\documentclass[12pt]{article}

\usepackage{natbib}
\usepackage{apalike}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}

\usepackage[shortlabels]{enumitem}
\usepackage[colorlinks=]{hyperref}
\usepackage[margin=2cm]{geometry}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=b,
    keepspaces=true,
    numbers=left,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    showtabs=false,
    tabsize=2
}

\lstset{style=mystyle}

\title{Worksheet: Reinforcement Learning}
\author{Jesse Geerts and Joaquin Rapela}

\begin{document}

\maketitle

\section{Complete notebook}

Please answer all questions in the
\href{https://github.com/jessegeerts/RLforNeuroTutorial/blob/main/ReinforcementLearningDemo.ipynb}{notebook}
that we discuss in the last practical session.

\section{Bayesian generalisation of the Rescorla-Wagner model}

One objective of this exercise is to check if the code implementing the
Bayesian generalisation of the Rescola-Wagner model in the previous notebook is
correct.

On slide 23 of the
\href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ldsLecture/ldsForNeuro.pdf#page=23}{practical
session on linear dynamical systems} we named, and assigned meaning to, all
variables of the Kalman filtering algorithm. Another objective of this exercise
is to assign a meaning to the variables in the function
\texttt{kalman\_filter\_update} of the above notebook. For example, we want to
know which variable in this function corresponds to the filtered mean or the
filtered covariance.

On slide 7 of the
\href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ldsLecture/ldsForNeuro.pdf#page=7}{practical
session on linear dynamical systems} we presented the equations that describe
the linear dynamical system model.

\begin{alignat}{4}
    \mathbf{x}_n &= A \mathbf{x}_{n-1}+\mathbf{w}_n\quad && \mathbf{w}_n\sim
    N(\mathbf{w}_n|\mathbf{0}, Q)\quad &&
    \mathbf{x}_n\in\mathbb{R}^M&&\nonumber\\
    \mathbf{y}_n &= C \mathbf{x}_{n}+\mathbf{v}_n\quad && \mathbf{v}_n\sim
    N(\mathbf{v}_n|\mathbf{0}, R)\quad && \mathbf{y}_n\in\mathbb{R}^P&&\quad
    n=1\ldots N\label{eq:lds}\\
    \mathbf{x}_0 &\sim N(\mathbf{w}_n|\mathbf{m}_0, V_0) && &&\nonumber
\end{alignat}

On slide 8 of the
\href{https://github.com/joacorapela/neuroinformatics24/blob/master/lectures/10_reinforcementLearning/RLinTheBrain_SWC_2024.pdf#page=8}{reinforcement
learning lesson} we provided the equations that describe the Bayesian
generalisation of the Rescorla-Wagner model. These equations can be rewritten
as

\begin{alignat}{4}
    \mathbf{w}_n &= \mathbf{w}_{n-1}+\mathbf{p}_n\quad && \mathbf{p}_n\sim
    N(\mathbf{p}_n|\mathbf{0}, \tau^2I_M)\quad &&
    \mathbf{w}_n\in\mathbb{R}^M&&\nonumber\\
    r_n &= x_n^T \mathbf{w}_{n}+\mathbf{q}_n\quad && \mathbf{q}_n\sim
    N(\mathbf{q}_n|\mathbf{0}, \sigma^2_r)\quad &&
    r_n\in\mathbb{R}&&\quad n=1\ldots N\label{eq:rw}\\
    \mathbf{w}_0 &\sim N(\mathbf{w}_0|\mathbf{0}, \sigma^2I_M) && &&\nonumber
\end{alignat}

Therefore we observe that:

\begin{enumerate}
    \item $\mathbf{x}_n$ in Eq.~\ref{eq:lds} correspond to $\mathbf{w}_n$ in Eq.~\ref{eq:rw}.
    \item $A$ in Eq.~\ref{eq:lds} correspond to $I_M$ in Eq.~\ref{eq:rw}.
    \item $\mathbf{w}_n$ in Eq.~\ref{eq:lds} correspond to $\mathbf{p}_n$ in Eq.~\ref{eq:rw}.
    \item $Q$ in Eq.~\ref{eq:lds} correspond to $\tau^2I_M$ in Eq.~\ref{eq:rw}.
    \item \ldots
\end{enumerate}

On slide 22 of the
\href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ldsLecture/ldsForNeuro.pdf#page=22}{practical
session on linear dynamical systems} we introduced the prediction and
filtering inference problems solved by the Kalman filter algorithm.

\begin{description}
    \item[Prediction]
        \begin{align}
            P(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n-1})=N(\mathbf{x}_n|\mathbf{x}_{n|n-1},P_{n|n-1})\nonumber
        \end{align}
    \item[Filtering]
        \begin{align}
            P(\mathbf{x}_n|\mathbf{y}_1,\ldots,\mathbf{y}_{n})=N(\mathbf{x}_n|\mathbf{x}_{n|n},P_{n|n})\nonumber
        \end{align}
\end{description}

Similarly, for the Bayesian generalisation of the Rescorla-Wagner model we can
define the following prediction and filtering problems

\begin{description}
    \item[Prediction]
        \begin{align}
            P(\mathbf{w}_n|r_1,\ldots,r_{n-1})=N(\mathbf{w}_n|\mathbf{w}_{n|n-1},P_{n|n-1})\nonumber
        \end{align}
    \item[Filtering]
        \begin{align}
            P(\mathbf{w}_n|r_1,\ldots,r_{n})=N(\mathbf{w}_n|\mathbf{w}_{n|n},P_{n|n})\nonumber
        \end{align}
\end{description}

On slide 23 of the
\href{https://www.gatsby.ucl.ac.uk/~rapela/neuroinformatics/2023/ldsLecture/ldsForNeuro.pdf#page=22}{practical
session on linear dynamical systems} we provided the recursive equations used by the
Kalman filter algorithm to estimate the prediction means and covariances,
$\mathbf{x}_{n|n-1},P_{n|n-1}$, and the filtering means and covariances,
$\mathbf{x}_{n|n},P_{n|n}$.

\begin{alignat*}{2}
    \mathbf{x}_{0|0}&=\mathbf{m}_0&&\text{init filtered mean}\\
    P_{0|0}&=V_0&&\text{init filtered covariance}\\
    \mathbf{x}_{n+1|n}&=A\mathbf{x}_{n|n}\quad&&\text{prediction mean}\\
    P_{n+1|n}&=AP_{n|n}A^\intercal+Q\quad&&\text{prediction covariance}\\
    \mathbf{y}_{n|n-1}&=C\mathbf{x}_{n|n-1}\quad&&\text{predicted observation}\\
    \tilde{\mathbf{y}}_n&=\textcolor{red}{\mathbf{y}_n}-\mathbf{y}_{n|n-1}\quad&&\text{residual}\\
    S_n&=CP_{n|n-1}C^\intercal+R\quad&&\text{residual covariance}\\
    \mathbf{x}_{n|n}&=\mathbf{x}_{n|n-1}+K_n\tilde{\mathbf{y}}_n\quad&&\text{filtering mean}\\
    K_n&=P_{n|n-1}C^\intercal S_n^{-1}&&\text{Kalman gain}\\
    P_{n|n}&=(I_M-K_nC)P_{n|n-1}&&\text{filtering covariance}
\end{alignat*}

Exercises:

\begin{enumerate}
    \item based on the previous correspondences and Kalman filter algorithm equations write down the equations of the Kalman
        filter algorithm to estimate the prediction means and covariances,
        $\mathbf{w}_{n|n-1},P_{n|n-1}$, and the filtering means and
        covariances, $\mathbf{w}_{n|n},P_{n|n}$, of the Bayesian extension of
        the Rescorla-Wagner model.

    \item find out what each line is doing in the function
        \texttt{kalman\_filter\_update}
        of the
        \href{https://github.com/jessegeerts/RLforNeuroTutorial/blob/main/ReinforcementLearningDemo.ipynb}{notebook}
        (e.g., the first line is computing the predictive covariance $P_{n|n}$)

    \item determine what are the inputs and outputs of the function
        \texttt{kalman\_filter\_update} in terms of variables of the Kalman
        filter algorithm (e.g., the input \texttt{meanw} is the filtered mean at time
        $n-1$ and the output \texttt{meanw} is the filtered mean at time $n$).

    \item is the function \texttt{kalman\_filter\_update} a correct
        implementation of the Kalman filter? Please explain in detail why the
        calculation of the Kalman gain is correct or incorrect.

\end{enumerate}

% \bibliographystyle{plainnat}
% \bibliography{tracking}

\end{document}
